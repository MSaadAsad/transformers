{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwHQlKyqtd2r"
      },
      "source": [
        "## **Load Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0LyXiSqp4tt"
      },
      "source": [
        "Since training would require a lot of data and Spotify is only so generous with their API, I decided to use an external song lyrics dataset from [Kaggle]((https://www.kaggle.com/datasets/terminate9298/songs-lyrics?select=lyrics.csv) ) that includes lyrics from over 25,000 songs, mostly in English, by 500 unique artists. The dataset contains a total of 7,619,966 unique words, providing sufficient data to train a small language model. I added artist names to the dataset hoping this would help the model learn specific artistic styles, thereby enhancing its ability to generate artist-specific lyrics upon inference. I employed byte-pair encoding to tokenize the dataset, which groups together frequently co-occurring characters. The total number of unique words in my dataset was just under 5,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-mP1u6_q5FM",
        "outputId": "a18d5d44-e67d-4c02-c5b1-20093b168a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install bpemb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Ujr-8mtuSoA"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import prune\n",
        "import math\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "from bpemb import BPEmb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6y1fWw4kXPim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f00ac69-8be0-49b2-f6a1-d0fbc5bbb2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tokens: 7619966\n",
            "Maximum number of tokens in 'formatted': 3708\n",
            "Number of unique artists: 542\n"
          ]
        }
      ],
      "source": [
        "# Load data from CSV, ignoring malformed lines\n",
        "df = pd.read_csv(\"/content/lyrics.csv\", on_bad_lines='skip')\n",
        "\n",
        "# Remove the 'link' column as it's not needed\n",
        "df = df.drop(columns=['link'])\n",
        "\n",
        "# Calculate number of words in 'lyrics' assuming space as delimiter\n",
        "df['token_count'] = df['lyrics'].apply(lambda x: len(x.split()))\n",
        "print(\"Total number of tokens:\", df['token_count'].sum())\n",
        "\n",
        "# Clean the lyrics text using a predefined function\n",
        "df['artist'] = df['artist'].str.replace(' Lyrics', '')\n",
        "df['formatted'] = df.apply(lambda row: row['artist'] + (\" \" + row['lyrics'] if row['lyrics'] else \"\"), axis=1)\n",
        "df['formatted_token_count'] = df['formatted'].apply(lambda x: len(x.split()))\n",
        "max_formatted_tokens = df['formatted_token_count'].max()\n",
        "print(\"Maximum number of tokens in 'formatted':\", max_formatted_tokens)\n",
        "\n",
        "# Determine and print the number of unique artists\n",
        "unique_artists = df['artist'].nunique()\n",
        "print(\"Number of unique artists:\", unique_artists)\n",
        "\n",
        "# Convert the 'formatted' column to lowercase\n",
        "df['formatted'] = df['formatted'].str.lower()\n",
        "\n",
        "# Remove unneeded columns to tidy up the DataFrame\n",
        "df = df.drop(columns=['artist', 'token_count', 'lyrics', 'song_name', 'Unnamed: 0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TxNcpUqhvNJA"
      },
      "outputs": [],
      "source": [
        "class BPETokenizer:\n",
        "    \"\"\"\n",
        "    Tokenizer based on Byte-Pair Encoding (BPE) to manage vocabulary and encoding/decoding of text.\n",
        "    \"\"\"\n",
        "    def __init__(self, lang=\"en\", vs=20000):\n",
        "        \"\"\"\n",
        "        Initialize the BPE tokenizer with specified language and vocabulary size, adding special tokens.\n",
        "        \"\"\"\n",
        "        self.bpemb = BPEmb(lang=lang, vs=vs)\n",
        "        self.vocab_size = self.bpemb.vocab_size + 2  # Account for PAD and UNK tokens\n",
        "\n",
        "        # Mapping from words to indices, offset by 2 to accommodate special tokens\n",
        "        self.word_index = {word: idx + 2 for idx, word in enumerate(self.bpemb.words)}\n",
        "        self.word_index[\"<PAD>\"] = 0\n",
        "        self.word_index[\"<UNK>\"] = 1\n",
        "\n",
        "        # Reverse mapping from indices to words\n",
        "        self.index_word = {idx: word for word, idx in self.word_index.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        Convert text to a list of indices. Unknown words are mapped to <UNK>.\n",
        "        \"\"\"\n",
        "        return [self.word_index.get(word, 1) for word in self.bpemb.encode(text)]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"\n",
        "        Convert a list of indices back to text, skipping special tokens.\n",
        "        \"\"\"\n",
        "        return ''.join(self.index_word.get(idx, \"<UNK>\") for idx in indices if idx > 1)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"\n",
        "        Return the total vocabulary size, including special tokens.\n",
        "        \"\"\"\n",
        "        return self.vocab_size\n",
        "\n",
        "\n",
        "class TransformersTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for text data prepared for training transformer models, handling tokenization and segmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, seq_length, lang=\"en\", vs=20000):\n",
        "        \"\"\"\n",
        "        Initialize dataset with texts, sequence length, and tokenizer parameters.\n",
        "        \"\"\"\n",
        "        self.tokenizer = BPETokenizer(lang=lang, vs=vs)\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenized_segments = self.tokenize_and_segment_texts(texts)\n",
        "\n",
        "    def tokenize_and_segment_texts(self, texts):\n",
        "        \"\"\"\n",
        "        Tokenize input texts and split into fixed-size segments, padding as needed.\n",
        "        \"\"\"\n",
        "        tokenized_segments = []\n",
        "        pad_id = self.tokenizer.word_index[\"<PAD>\"]\n",
        "        for text in texts:\n",
        "            encoded = self.tokenizer.encode(text)\n",
        "            padded_length = (len(encoded) + self.seq_length - 1) // self.seq_length * self.seq_length\n",
        "            encoded.extend([pad_id] * (padded_length - len(encoded)))\n",
        "\n",
        "            chunks = [encoded[i:i + self.seq_length] for i in range(0, len(encoded), self.seq_length)]\n",
        "            tokenized_segments.extend(chunks)\n",
        "        return tokenized_segments\n",
        "\n",
        "    def unique_words(self):\n",
        "        \"\"\"\n",
        "        Calculate the number of unique words in the tokenized segments, excluding special tokens.\n",
        "        \"\"\"\n",
        "        return len(set(word for segment in self.tokenized_segments for word in segment if word > 1))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of segments in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.tokenized_segments)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve a pair of input-target sequences from the dataset by index.\n",
        "        \"\"\"\n",
        "        segment = self.tokenized_segments[idx]\n",
        "        return torch.tensor(segment[:-1], dtype=torch.long), torch.tensor(segment[1:], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "82ePERSE4gkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f338be45-b4c9-47de-8e45-8a38719806b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs5000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 315918/315918 [00:00<00:00, 738046.04B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs5000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1888515/1888515 [00:00<00:00, 2197596.55B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dataset: 4859\n",
            "Vocabulary size of tokenizer: 5002\n",
            "Training dataset size: 52744\n",
            "Test dataset size: 13187\n"
          ]
        }
      ],
      "source": [
        "texts = df['formatted'].tolist()\n",
        "\n",
        "dataset = TransformersTextDataset(texts, seq_length=256, vs=5000)\n",
        "print(f'Number of unique words in dataset: {dataset.unique_words()}')\n",
        "print(f'Vocabulary size of tokenizer: {dataset.tokenizer.get_vocab_size()}')\n",
        "\n",
        "# Calculate train/test split sizes\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Randomly split dataset into training and testing parts\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Output sizes of the splits\n",
        "print(f'Training dataset size: {len(train_dataset)}')\n",
        "print(f'Test dataset size: {len(test_dataset)}')\n",
        "\n",
        "# Create DataLoaders for batch processing of training and testing datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CjZ1AghvlU3"
      },
      "outputs": [],
      "source": [
        "# Use GPU if available (I used L4 from Google Colab)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRiGODMEgmyW"
      },
      "source": [
        "## **GQA Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igxo72OgrONz"
      },
      "source": [
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "The efficiency of transformer models is largely governed by the computation of the attention matrix, where each query is dot-producted with a key, comparing each token to every other token within the layer. This results in a computational complexity of $O(n^2)$, where $n$ is the dimension of the model, and becomes a major bottleneck when training larger models, particularly those with more heads or larger context sizes.\n",
        "\n",
        "Techniques such as sparse attention and flash attention are designed to tackle this $O(n^2)$ scaling have already been used in the past. Here we will look at two very recent techniques GQA and MoD.\n",
        "In [grouped query attention](https://arxiv.org/pdf/2305.13245.pdf), instead of maintaining a unique key matrix for each head in a standard multi-head layer, we use fewer key matrices, with multiple query matrices interacting with the same key. This configuration reduces the number of parameters and accelerates training time. By grouping the queries into $G$ groups, we effectively reduce complexity; when $G$ equals the number of heads, the architecture reverts to that of a standard transformer. Changing the number of Heads is a tradeoff between speed and accuracy, with lower numbers leading to signifiantly faster run times yet with poorer accuracy.\n",
        "\n",
        "For my implementation, I adapted a component from the in-development [grouped-query-attention-library](https://github.com/fkodom/grouped-query-attention-pytorch/blob/main/README.md) to include a Grouped Query (GQ) Attention block in each transformer layer, replacing the conventional multi-head setup. This modification has resulted in a faster training process, albeit at a slight compromise in accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G9eRrfuj2p0"
      },
      "source": [
        "### **Existing Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvRjQIifSh5Z",
        "outputId": "334eb115-729b-4065-8ffd-050f0aff9867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wW0ga4ij5Sz"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple, Union\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import einsum, rearrange\n",
        "from torch import Tensor, nn\n",
        "\n",
        "def scaled_dot_product_gqa(\n",
        "    query: Tensor,\n",
        "    key: Tensor,\n",
        "    value: Tensor,\n",
        "    dropout: float = 0.1,\n",
        "    scale: Optional[float] = None,\n",
        "    mask: Optional[Tensor] = None,\n",
        "    is_causal: Optional[bool] = True,\n",
        "    need_weights: bool = False,\n",
        "    average_attn_weights: bool = False,\n",
        "    force_grouped: bool = False,\n",
        "):\n",
        "    \"\"\"Scaled dot product attention with support for grouped queries.\n",
        "\n",
        "    Einstein notation:\n",
        "    - b: batch size\n",
        "    - n / s: sequence length\n",
        "    - h: number of heads\n",
        "    - g: number of groups\n",
        "    - d: dimension of query/key/value\n",
        "\n",
        "    Args:\n",
        "        query: Query tensor of shape (b, n, h, d)\n",
        "        key: Key tensor of shape (b, s, h, d)\n",
        "        value: Value tensor of shape (b, s, h, d)\n",
        "        dropout: Dropout probability (default: 0.0)\n",
        "        scale: Scale factor for query (default: d_query ** 0.5)\n",
        "        mask: Mask tensor of shape (b, n, s) or (b, s). If 'ndim == 2', the mask is\n",
        "            applied to all 'n' rows of the attention matrix. (default: None)\n",
        "        force_grouped: If True, apply grouped-query attention even if the number of\n",
        "            heads is equal for query, key, and value. (default: False)\n",
        "\n",
        "    Returns:\n",
        "        2-tuple of:\n",
        "        - Attention output with shape (b, n, h, d)\n",
        "        - (Optional) Attention weights with shape (b, h, n, s). Only returned if\n",
        "          'need_weights' is True.\n",
        "    \"\"\"\n",
        "    if (mask is not None) and (is_causal is not None):\n",
        "        raise ValueError(\n",
        "            \"Only one of 'mask' and 'is_causal' should be provided, but got both.\"\n",
        "        )\n",
        "    elif not query.ndim == key.ndim == value.ndim == 4:\n",
        "        raise ValueError(\n",
        "            f\"Expected query, key, and value to be 4-dimensional, but got shapes \"\n",
        "            f\"{query.shape}, {key.shape}, and {value.shape}.\"\n",
        "        )\n",
        "\n",
        "    # Move sequence length dimension to axis 2.\n",
        "    # This makes the attention operations below *much* faster.\n",
        "    query = rearrange(query, \"b n h d -> b h n d\")\n",
        "    key = rearrange(key, \"b s h d -> b h s d\")\n",
        "    value = rearrange(value, \"b s h d -> b h s d\")\n",
        "\n",
        "    bq, hq, nq, dq = query.shape\n",
        "    bk, hk, nk, dk = key.shape\n",
        "    bv, hv, nv, dv = value.shape\n",
        "    if not (bq == bk == bv and dq == dk == dv):\n",
        "        raise ValueError(\n",
        "            \"Expected query, key, and value to have the same batch size (dim=0) and \"\n",
        "            f\"embedding dimension (dim=3), but got query: {query.shape}, \"\n",
        "            f\"key: {key.shape}, and value: {value.shape}.\"\n",
        "        )\n",
        "    elif (hk != hv) or (nk != nv):\n",
        "        raise ValueError(\n",
        "            \"Expected key and value to have the same size in dimensions 1 and 2, but \"\n",
        "            f\"got key: {key.shape} and value: {value.shape}.\"\n",
        "        )\n",
        "    elif hq % hk != 0:\n",
        "        raise ValueError(\n",
        "            \"Expected query heads to be a multiple of key/value heads, but got \"\n",
        "            f\"query: {query.shape} and key/value: {key.shape}.\"\n",
        "        )\n",
        "\n",
        "    if scale is None:\n",
        "        scale = query.size(-1) ** 0.5\n",
        "    query = query / scale\n",
        "\n",
        "    num_head_groups = hq // hk\n",
        "    if num_head_groups > 1 or force_grouped:\n",
        "        # Separate the query heads into 'num_head_groups' chunks, and fold the group\n",
        "        # dimension into the batch dimension.  This allows us to compute the attention\n",
        "        # for each head in parallel, then sum over all of the groups at the end.\n",
        "        query = rearrange(query, \"b (h g) n d -> b g h n d\", g=num_head_groups)\n",
        "        similarity = einsum(query, key, \"b g h n d, b h s d -> b h n s\")\n",
        "    else:\n",
        "        # If the number of query/key heads is equal, we can skip grouping the queries,\n",
        "        # and just use the standard sdot product attention.\n",
        "        similarity = einsum(query, key, \"b h n d, b h s d -> b h n s\")\n",
        "\n",
        "    if is_causal:\n",
        "        # Mask out the upper triangular portion of the attention matrix. This prevents\n",
        "        # the model from attending to tokens in the future.\n",
        "        mask = torch.ones(\n",
        "            (bq, nq, nk),\n",
        "            device=query.device,\n",
        "            dtype=torch.bool,\n",
        "        ).tril_()\n",
        "\n",
        "    if mask is not None:\n",
        "        # Expand mask to match the shape of the attention matrix.\n",
        "        # If mask is 2D, assume that it is applied to the key/value sequence dimension.\n",
        "        # Else if mask is 3D, assume that it is applied to the query/key/value sequence\n",
        "        # dimension for all attention heads.\n",
        "        #\n",
        "        # Users could also provide a 4D mask, which is applied to the query/key/value\n",
        "        # sequence dimension for each attention head (though I don't have a particular\n",
        "        # use case in mind for that).\n",
        "        if mask.ndim == 2:\n",
        "            mask = rearrange(mask, \"b s -> b () () s\")\n",
        "        elif mask.ndim == 3:\n",
        "            mask = rearrange(mask, \"b n s -> b () n s\")\n",
        "        # Mask similarity values by setting them to negative infinity.  This guarantees\n",
        "        # that they will not contribute to the softmax computation below.\n",
        "        similarity.masked_fill_(~mask, torch.finfo(similarity.dtype).min)\n",
        "\n",
        "    attention = F.softmax(similarity / scale, dim=-1)\n",
        "    if 0.1 > 0.0:\n",
        "        attention = F.dropout(attention, p=0.1)\n",
        "\n",
        "    # Apply attention matrix to the value Tensor.\n",
        "    out = einsum(attention, value, \"b h n s, b h s d -> b h n d\")\n",
        "    # Move head dimension back to axis 2\n",
        "    out = rearrange(out, \"b h n d -> b n h d\")\n",
        "\n",
        "    attn_weights: Optional[Tensor] = None\n",
        "    if need_weights:\n",
        "        # Move the sequence dimensions back to positions 1, 2.  Move the head dimension\n",
        "        # to position 3.  This more closely matches the return shape of the attention\n",
        "        # output: (b, n, h, d).\n",
        "        attn_weights = rearrange(attention, \"b h n s -> b n s h\")\n",
        "        if average_attn_weights:\n",
        "            attn_weights = attn_weights.mean(dim=1)\n",
        "\n",
        "    return out, attn_weights\n",
        "\n",
        "\n",
        "class MultiheadGQA(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        query_heads: int,\n",
        "        kv_heads: int,\n",
        "        dropout: float = 0.1,\n",
        "        bias: bool = True,\n",
        "        layer_norm: bool = True,\n",
        "        layer_norm_eps: float = 1e-5,\n",
        "        gamma_init: float = 1.0,\n",
        "        device: Optional[Union[torch.device, str]] = None,\n",
        "        dtype: Optional[torch.dtype] = None,\n",
        "        batch_first: bool = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.query_heads = query_heads\n",
        "        self.kv_heads = kv_heads\n",
        "        self.dropout = dropout\n",
        "        self.layer_norm = layer_norm\n",
        "        self.gamma_init = gamma_init\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        if self.query_heads % self.kv_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"query_heads ({query_heads}) must be divisible by \"\n",
        "                f\"kv_heads ({kv_heads})\"\n",
        "            )\n",
        "        elif (embed_dim % self.query_heads != 0) or (embed_dim % self.kv_heads != 0):\n",
        "            raise ValueError(\n",
        "                f\"embed_dim ({embed_dim}) must be divisible by \"\n",
        "                f\"query_heads ({query_heads}) and kv_heads ({kv_heads})\"\n",
        "            )\n",
        "\n",
        "        head_dim = embed_dim // query_heads\n",
        "        if not head_dim % 8 == 0:\n",
        "            raise ValueError(\n",
        "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be divisible by 8\"\n",
        "            )\n",
        "        if not head_dim <= 128:\n",
        "            raise ValueError(\n",
        "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be <= 128\"\n",
        "            )\n",
        "\n",
        "        # Query projection layer is the same as in vanilla MHA.\n",
        "        self.q_proj = nn.Linear(\n",
        "            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
        "        )\n",
        "        # Key/value projection layers have a smaller output dimension, so that\n",
        "        # the we have fewer key/value attention heads after reshaping.\n",
        "        kv_embed_dim = embed_dim // query_heads * kv_heads\n",
        "        self.k_proj = nn.Linear(\n",
        "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
        "        )\n",
        "        self.v_proj = nn.Linear(\n",
        "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
        "        )\n",
        "        self.norm: Optional[nn.LayerNorm] = None\n",
        "        if layer_norm:\n",
        "            self.norm = nn.LayerNorm(\n",
        "                kv_embed_dim, eps=layer_norm_eps, device=device, dtype=dtype\n",
        "            )\n",
        "\n",
        "        self.out_proj = nn.Linear(\n",
        "            kv_embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
        "        )\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.xavier_normal_(self.q_proj.weight)\n",
        "        if self.q_proj.bias is not None:\n",
        "            nn.init.constant_(self.q_proj.bias, 0)\n",
        "        nn.init.xavier_normal_(self.k_proj.weight)\n",
        "        if self.k_proj.bias is not None:\n",
        "            nn.init.constant_(self.k_proj.bias, 0)\n",
        "\n",
        "        nn.init.xavier_normal_(self.v_proj.weight, gain=self.gamma_init)\n",
        "        if self.v_proj.bias is not None:\n",
        "            nn.init.constant_(self.v_proj.bias, 0)\n",
        "        nn.init.xavier_normal_(self.out_proj.weight, gain=self.gamma_init)\n",
        "        if self.out_proj.bias is not None:\n",
        "            nn.init.constant_(self.out_proj.bias, 0)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        key: Tensor,\n",
        "        value: Tensor,\n",
        "        need_weights: bool = False,\n",
        "        # TODO\n",
        "        # attn_mask: Optional[Tensor] = None,\n",
        "        is_causal: bool = True,\n",
        "        average_attn_weights: bool = False,\n",
        "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        # Notation:\n",
        "        #   b - batch size\n",
        "        #   n - sequence length\n",
        "        #   h - number of heads\n",
        "        #   d - embedding dimension\n",
        "        #\n",
        "        # Input shape: (b, n, d)\n",
        "        q: Tensor = self.q_proj(query)\n",
        "        k: Tensor = self.k_proj(key)\n",
        "        v: Tensor = self.v_proj(value)\n",
        "\n",
        "        # Unfold 'd' dimension into 'h' separate attention heads.\n",
        "        q = rearrange(q, \"b n (h d) -> b n h d\", h=self.query_heads)\n",
        "        k = rearrange(k, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
        "        v = rearrange(v, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
        "        # Apply attention, then fold 'h' attention heads back into 'd'.\n",
        "        x, attn = scaled_dot_product_gqa(\n",
        "            query=q,\n",
        "            key=k,\n",
        "            value=v,\n",
        "            # TODO\n",
        "            # mask=attn_mask,\n",
        "            is_causal=is_causal,\n",
        "            need_weights=need_weights,\n",
        "            average_attn_weights=average_attn_weights,\n",
        "            force_grouped=False,\n",
        "        )\n",
        "        x = rearrange(x, \"b n h d -> b n (h d)\")\n",
        "\n",
        "        if self.layer_norm:\n",
        "            assert self.norm is not None\n",
        "            x = self.norm(x)\n",
        "        # Linear projection on attention outputs.\n",
        "        x = self.out_proj(x)\n",
        "\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0DkFcA2onyg"
      },
      "source": [
        "## **MoD Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAoYYTa1reLv"
      },
      "source": [
        "A recent advancement aimed at addressing the scalability challenges of the self-attention layer is the [Mixture of Depths](https://arxiv.org/pdf/2404.02258.pdf) routing. This technique involves a routing layer that learns to select only the top p most effective tokens for processing, with research suggesting that the optimal value for p is 12.5%. Tokens not selected are passed directly to the subsequent layer via a residual connection. This approach increases the number of parameters linearly with n, where n represents the number of tokens in an attention head, but it reduces the computational load by some factor of n^2, enhancing computational efficiency but also learning to pay attention intelligently. The resulting network generally achieves a loss comparable to that of a slower, conventional vanilla network. Typically, this routing is applied every other layer to preserve essential information. In my project, I adapted an implementation by [George Grigorev](https://github.com/thepowerfuldeez/OLMo/blob/main/olmo/mod.py) from GitHub to successfully employ this technique, which significantly accelerated the training process. The implementation uses a MoD wrapper layer which applies the routing before passing the tokens to a standard transformer block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgclGQv826GS"
      },
      "source": [
        "### **Existing Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywhf5gQVWdLU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVQbtu7sPLTw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MoD(nn.Module):\n",
        "    \"\"\"The Mixtures of Depth Block that dynamically selects which tokens to process in a block.\n",
        "    Wraps around a decoder block to allow for token dropping, optimizing computational resources.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, block):\n",
        "        super().__init__()\n",
        "        self.block = block  # block is attention + MLP\n",
        "        self.capacity_factor = config.mod_capacity_factor\n",
        "        self.top_k = int(self.capacity_factor * config.max_sequence_length)\n",
        "\n",
        "        if self.capacity_factor < 1:\n",
        "            # Initialize routers only if capacity factor is less than 1\n",
        "            self.mod_router = nn.Linear(config.d_model, 1, bias=False)\n",
        "            self.mlp_router = nn.Linear(config.d_model, 1, bias=False)\n",
        "\n",
        "        # BCE loss is used for inference\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.block.reset_parameters()\n",
        "        if self.capacity_factor < 1:\n",
        "            self.mod_router.reset_parameters()\n",
        "            self.mlp_router.reset_parameters()\n",
        "\n",
        "    def set_activation_checkpointing(self, strategy):\n",
        "        self.block.set_activation_checkpointing(strategy)\n",
        "\n",
        "    def get_aux_loss(self, x, targets):\n",
        "        if self.capacity_factor < 1:\n",
        "            B, T, C = x.shape\n",
        "            mlp_router_logits = self.mlp_router(x.detach().view(B * T, -1))\n",
        "            return self.bce_loss(mlp_router_logits.view(-1), targets)\n",
        "        else:\n",
        "            return torch.tensor(0.0).to(x.device)\n",
        "\n",
        "    def forward(self, x, memory=None, **kwargs):\n",
        "        B, T, C = x.shape\n",
        "        if self.capacity_factor < 1:\n",
        "            top_k = min(self.top_k, int(self.capacity_factor * T))\n",
        "\n",
        "            \"\"\"STEP 1: get logits and top_k tokens\"\"\"\n",
        "            router_logits = self.mod_router(x)\n",
        "            weights, selected_tokens = torch.topk(router_logits, top_k, dim=1, sorted=False)\n",
        "\n",
        "            # 0, if not in topk tokens, 1 else\n",
        "            mlp_targets = torch.zeros_like(router_logits).view(-1)\n",
        "            mlp_targets[selected_tokens.view(-1)] = 1.0\n",
        "            aux_loss = self.get_aux_loss(x, mlp_targets)\n",
        "\n",
        "            # IMPORTANT: need to sort indices to keep causal order for those tokens that\n",
        "            # are processed in a block\n",
        "            selected_tokens, index = torch.sort(selected_tokens, dim=1)\n",
        "            weights = torch.gather(weights, dim=1, index=index)\n",
        "\n",
        "            \"\"\"STEP 2: expand indices to process batches with _reduced_ seqlen\"\"\"\n",
        "            indices_expanded = selected_tokens.expand(-1, -1, C)\n",
        "            top_k_tokens = torch.gather(x, 1, indices_expanded)\n",
        "\n",
        "            # Make sure to pass both 'tgt' and 'memory' to the TransformerDecoderLayer\n",
        "            if memory is None:\n",
        "                memory = top_k_tokens  # Use self-attention if no external memory is provided\n",
        "\n",
        "            top_k_tokens_processed, cache = self.block(top_k_tokens, memory, **kwargs)\n",
        "\n",
        "            \"\"\"STEP 3: combine results\"\"\"\n",
        "            x = torch.scatter_add(\n",
        "                x,\n",
        "                dim=1,\n",
        "                index=indices_expanded,\n",
        "                src=top_k_tokens_processed * weights,\n",
        "            )\n",
        "            return x, cache, aux_loss\n",
        "        else:\n",
        "            # When capacity factor is 1, process all tokens as a vanilla Transformer layer\n",
        "            x_processed, cache = self.block(x, memory, **kwargs)\n",
        "            return x_processed, cache, torch.tensor(0.0).to(x.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxlvzCP1Gjf7"
      },
      "source": [
        "## **Frankestein**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceL9-5rProqG"
      },
      "source": [
        "Since both applications are compatible, I wanted to know if they could be implemented in a single architecture with faster speed or better performance. I decided to create a “Frankenstein” using these. Since MoD accepts any block to wrap, I decided to create a transformer block with GQ Attention instead of multi-head and decided to pass it.\n",
        "\n",
        "Here is a comparison between the standard approach and my hybrid. Standard:\n",
        "\n",
        "\n",
        "```\n",
        "  Input: tokens, original_dim = n\n",
        "  Output: softmax output of dimensions matching vocab_size\n",
        "\n",
        "  For each layer in model_layers:\n",
        "      original_tokens = tokens  # Save the input to the layer for the residual connection\n",
        "      tokens = layer_norm(tokens)  # Normalize the input tokens for the layer\n",
        "\n",
        "      head_outputs = []\n",
        "\n",
        "      For each head in num_heads:\n",
        "          Q = derive_query_matrix(tokens, dim_q)\n",
        "          K = derive_key_matrix(tokens, dim_k)\n",
        "          V = derive_value_matrix(tokens, dim_v)\n",
        "          head_output = attention(Q, K, V)  # Compute attention output\n",
        "          head_outputs.append(head_output)\n",
        "\n",
        "      tokens = concatenate(head_outputs)\n",
        "      tokens = sum(tokens)  # Sum the outputs from all heads\n",
        "      tokens += original_tokens  # Add the residual connection here\n",
        "\n",
        "      tokens = batch_norm(tokens)  # Normalize the combined output\n",
        "      original_tokens_ffn = tokens  # Save the output for the residual connection post-FFN\n",
        "      tokens = FFN(tokens)  # Apply the feed-forward network\n",
        "      tokens += original_tokens_ffn  # Add the residual connection after the FFN\n",
        "\n",
        "  tokens = proj(tokens, dim = vocab_size)  # Project output tokens to vocabulary size\n",
        "  output = softmax(tokens)  # Apply softmax to get probability distribution over vocabulary\n",
        "\n",
        "```\n",
        "\n",
        "Frankenstein Transformer:\n",
        "\n",
        "```\n",
        "Input: tokens, dim = n\n",
        "Output: softmax output of dimensions matching vocab_size\n",
        "\n",
        "For each layer in model_layers:\n",
        "    original_tokens = tokens  # Save the input to the layer for the residual connection\n",
        "\n",
        "    If layer_number % 2 == 0:\n",
        "        tokens = MoD-routing(tokens, p=1)  # Apply MoD-routing with p=1 (full routing) for even layers\n",
        "    Else:\n",
        "        tokens = MoD-routing(tokens, p=0.125)  # Apply MoD-routing with p=0.125 for odd layers\n",
        "\n",
        "    tokens = layer_norm(tokens)  # Normalize the inputs for the layer\n",
        "\n",
        "    head_outputs = []\n",
        "    For each head in num_heads:\n",
        "        Q = derive_query_matrix(tokens, dim_q)\n",
        "        K = derive_key_matrix(tokens, dim_k)\n",
        "        V = derive_value_matrix(tokens, dim_v)\n",
        "        head_output = grouped-query-attention(Q, K, V)  # Grouped-query attention where number of K, V <= Q\n",
        "        head_outputs.append(head_output)\n",
        "\n",
        "    tokens = concatenate(head_outputs)\n",
        "    tokens = sum(tokens)  # Sum the outputs from all heads\n",
        "    tokens += original_tokens  # Add the residual connection after attention sum\n",
        "\n",
        "    tokens = batch_norm(tokens)  # Normalize the combined output\n",
        "    original_tokens_ffn = tokens  # Save the output for the residual connection post-FFN\n",
        "    tokens = FFN(tokens)  # Apply the feed-forward network post normalization\n",
        "    tokens += original_tokens_ffn  # Add the residual connection after the FFN\n",
        "\n",
        "tokens = proj(tokens, dim = vocab_size)  # Project output tokens to vocabulary size\n",
        "output = softmax(tokens)  # Apply softmax to get probability distribution over vocabulary\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81vgi-vT8XLo"
      },
      "source": [
        "This is one of the first implementations of MoD since it has only come out recently, and the only implementation combining it with GQA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX3sz1ypKEqg"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayerGQA(nn.Module):\n",
        "    def __init__(self, embed_dim, query_heads, kv_heads, dim_feedforward=2048):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiheadGQA(embed_dim, query_heads, kv_heads)  # Using MultiheadGQA for self-attention\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, tgt, tgt_mask=None, tgt_key_padding_mask=None, tgt_is_causal=True):\n",
        "        # Here, memory and tgt are the same as we are in a decoder-only model\n",
        "        # Using tgt as both query, key, and value for self-attention\n",
        "        tgt2, _ = self.self_attn(tgt, tgt, tgt, need_weights=False, is_causal=tgt_is_causal)\n",
        "        tgt = tgt + self.norm1(tgt2)  # Apply residual connection followed by normalization\n",
        "        tgt2 = self.linear2(F.relu(self.linear1(tgt)))  # Feed-forward network\n",
        "        tgt = tgt + self.norm2(tgt2)  # Another residual connection and normalization\n",
        "        cache = None\n",
        "        return tgt, None\n",
        "\n",
        "class Frankenstein(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Frankenstein, self).__init__()\n",
        "        self.embedding = nn.Embedding(config.num_tokens, config.d_model)\n",
        "        self.pos_encoder = self.create_positional_encoding(config.max_sequence_length, config.d_model)\n",
        "        # Layers wrapped in MoD\n",
        "        self.layers = nn.ModuleList([\n",
        "            MoD(config if i % 2 == 0 else self.modify_config(config, config.mod_reduced),\n",
        "                TransformerDecoderLayerGQA(config.d_model, config.query_heads, config.kv_heads, config.dim_feedforward))\n",
        "            for i in range(config.num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(config.d_model)\n",
        "        self.final_linear = nn.Linear(config.d_model, config.num_tokens)\n",
        "\n",
        "    def modify_config(self, config, mod_capacity_factor):\n",
        "        config_copy = config.__class__(**{k: v for k, v in vars(config).items()})\n",
        "        setattr(config_copy, 'mod_capacity_factor', mod_capacity_factor)\n",
        "        return config_copy\n",
        "\n",
        "    def create_positional_encoding(self, length, d_model):\n",
        "        position = torch.arange(length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pos_encoder = torch.zeros(length, d_model)\n",
        "        pos_encoder[:, 0::2] = torch.sin(position * div_term)\n",
        "        pos_encoder[:, 1::2] = torch.cos(position * div_term)\n",
        "        return nn.Parameter(pos_encoder.unsqueeze(0))\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src) + self.pos_encoder[:, :src.size(1), :]\n",
        "        memory = None\n",
        "\n",
        "        aux_losses = []\n",
        "        for layer in self.layers:\n",
        "            src, cache, aux_loss = layer(src, memory)\n",
        "            aux_losses.append(aux_loss)\n",
        "\n",
        "        src = self.final_linear(self.norm(src))\n",
        "        return src, sum(aux_losses) / len(aux_losses) if aux_losses else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFrDfuUEKHNz"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    max_sequence_length = 256\n",
        "    num_tokens = 5002  # vocabulary size\n",
        "    d_model = 256  # dimensionality of the token embeddings\n",
        "    query_heads = 8  # number of query heads in multi-head attention mechanisms\n",
        "    kv_heads = 2  # number of key/value heads in multi-head attention mechanisms\n",
        "    dim_feedforward = 512  # dimension of the feedforward network model in transformer\n",
        "    num_layers = 4  # number of decoder layers\n",
        "    mod_capacity_factor = 1  # default capacity factor\n",
        "    mod_reduced = 0.125 # This amount of tokens in every other layer will pass through"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cRGFvd-Ytsg",
        "outputId": "5c0de544-462d-4cb7-df44-251423c6a499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Total Loss: 7.24, Auxiliary Loss: 519.90, Training Time: 48.519385 seconds\n",
            "Epoch 2, Total Loss: 7.06, Auxiliary Loss: 551.90, Training Time: 48.657121 seconds\n",
            "Epoch 3, Total Loss: 6.28, Auxiliary Loss: 565.11, Training Time: 48.700475 seconds\n"
          ]
        }
      ],
      "source": [
        "config = Config()\n",
        "model = Frankenstein(config).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 3\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    start_time = datetime.now()\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_aux_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, aux_loss = model(inputs)\n",
        "        output = output.reshape(-1, config.num_tokens)\n",
        "        targets = targets.reshape(-1)\n",
        "        loss = loss_fn(output, targets)\n",
        "        total_loss = loss.item() + aux_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses for logging\n",
        "        total_loss += loss.item()\n",
        "        total_aux_loss += aux_loss.item()\n",
        "\n",
        "    # Calculate training duration\n",
        "    end_time = datetime.now()\n",
        "    training_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Total Loss: {total_loss:.2f}, Auxiliary Loss: {total_aux_loss:.2f}, Training Time: {training_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5eARYsj7e0i",
        "outputId": "ed70ac87-a916-467e-c98e-e00d10a8b7ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pos_encoder has 16384 trainable parameters\n",
            "embedding.weight has 640256 trainable parameters\n",
            "layers.0.block.self_attn.q_proj.weight has 16384 trainable parameters\n",
            "layers.0.block.self_attn.q_proj.bias has 128 trainable parameters\n",
            "layers.0.block.self_attn.k_proj.weight has 4096 trainable parameters\n",
            "layers.0.block.self_attn.k_proj.bias has 32 trainable parameters\n",
            "layers.0.block.self_attn.v_proj.weight has 4096 trainable parameters\n",
            "layers.0.block.self_attn.v_proj.bias has 32 trainable parameters\n",
            "layers.0.block.self_attn.norm.weight has 32 trainable parameters\n",
            "layers.0.block.self_attn.norm.bias has 32 trainable parameters\n",
            "layers.0.block.self_attn.out_proj.weight has 4096 trainable parameters\n",
            "layers.0.block.self_attn.out_proj.bias has 128 trainable parameters\n",
            "layers.0.block.norm1.weight has 128 trainable parameters\n",
            "layers.0.block.norm1.bias has 128 trainable parameters\n",
            "layers.0.block.linear1.weight has 65536 trainable parameters\n",
            "layers.0.block.linear1.bias has 512 trainable parameters\n",
            "layers.0.block.linear2.weight has 65536 trainable parameters\n",
            "layers.0.block.linear2.bias has 128 trainable parameters\n",
            "layers.0.block.norm2.weight has 128 trainable parameters\n",
            "layers.0.block.norm2.bias has 128 trainable parameters\n",
            "layers.1.block.self_attn.q_proj.weight has 16384 trainable parameters\n",
            "layers.1.block.self_attn.q_proj.bias has 128 trainable parameters\n",
            "layers.1.block.self_attn.k_proj.weight has 4096 trainable parameters\n",
            "layers.1.block.self_attn.k_proj.bias has 32 trainable parameters\n",
            "layers.1.block.self_attn.v_proj.weight has 4096 trainable parameters\n",
            "layers.1.block.self_attn.v_proj.bias has 32 trainable parameters\n",
            "layers.1.block.self_attn.norm.weight has 32 trainable parameters\n",
            "layers.1.block.self_attn.norm.bias has 32 trainable parameters\n",
            "layers.1.block.self_attn.out_proj.weight has 4096 trainable parameters\n",
            "layers.1.block.self_attn.out_proj.bias has 128 trainable parameters\n",
            "layers.1.block.norm1.weight has 128 trainable parameters\n",
            "layers.1.block.norm1.bias has 128 trainable parameters\n",
            "layers.1.block.linear1.weight has 65536 trainable parameters\n",
            "layers.1.block.linear1.bias has 512 trainable parameters\n",
            "layers.1.block.linear2.weight has 65536 trainable parameters\n",
            "layers.1.block.linear2.bias has 128 trainable parameters\n",
            "layers.1.block.norm2.weight has 128 trainable parameters\n",
            "layers.1.block.norm2.bias has 128 trainable parameters\n",
            "layers.1.mod_router.weight has 128 trainable parameters\n",
            "layers.1.mlp_router.weight has 128 trainable parameters\n",
            "layers.2.block.self_attn.q_proj.weight has 16384 trainable parameters\n",
            "layers.2.block.self_attn.q_proj.bias has 128 trainable parameters\n",
            "layers.2.block.self_attn.k_proj.weight has 4096 trainable parameters\n",
            "layers.2.block.self_attn.k_proj.bias has 32 trainable parameters\n",
            "layers.2.block.self_attn.v_proj.weight has 4096 trainable parameters\n",
            "layers.2.block.self_attn.v_proj.bias has 32 trainable parameters\n",
            "layers.2.block.self_attn.norm.weight has 32 trainable parameters\n",
            "layers.2.block.self_attn.norm.bias has 32 trainable parameters\n",
            "layers.2.block.self_attn.out_proj.weight has 4096 trainable parameters\n",
            "layers.2.block.self_attn.out_proj.bias has 128 trainable parameters\n",
            "layers.2.block.norm1.weight has 128 trainable parameters\n",
            "layers.2.block.norm1.bias has 128 trainable parameters\n",
            "layers.2.block.linear1.weight has 65536 trainable parameters\n",
            "layers.2.block.linear1.bias has 512 trainable parameters\n",
            "layers.2.block.linear2.weight has 65536 trainable parameters\n",
            "layers.2.block.linear2.bias has 128 trainable parameters\n",
            "layers.2.block.norm2.weight has 128 trainable parameters\n",
            "layers.2.block.norm2.bias has 128 trainable parameters\n",
            "layers.3.block.self_attn.q_proj.weight has 16384 trainable parameters\n",
            "layers.3.block.self_attn.q_proj.bias has 128 trainable parameters\n",
            "layers.3.block.self_attn.k_proj.weight has 4096 trainable parameters\n",
            "layers.3.block.self_attn.k_proj.bias has 32 trainable parameters\n",
            "layers.3.block.self_attn.v_proj.weight has 4096 trainable parameters\n",
            "layers.3.block.self_attn.v_proj.bias has 32 trainable parameters\n",
            "layers.3.block.self_attn.norm.weight has 32 trainable parameters\n",
            "layers.3.block.self_attn.norm.bias has 32 trainable parameters\n",
            "layers.3.block.self_attn.out_proj.weight has 4096 trainable parameters\n",
            "layers.3.block.self_attn.out_proj.bias has 128 trainable parameters\n",
            "layers.3.block.norm1.weight has 128 trainable parameters\n",
            "layers.3.block.norm1.bias has 128 trainable parameters\n",
            "layers.3.block.linear1.weight has 65536 trainable parameters\n",
            "layers.3.block.linear1.bias has 512 trainable parameters\n",
            "layers.3.block.linear2.weight has 65536 trainable parameters\n",
            "layers.3.block.linear2.bias has 128 trainable parameters\n",
            "layers.3.block.norm2.weight has 128 trainable parameters\n",
            "layers.3.block.norm2.bias has 128 trainable parameters\n",
            "layers.3.mod_router.weight has 128 trainable parameters\n",
            "layers.3.mlp_router.weight has 128 trainable parameters\n",
            "norm.weight has 128 trainable parameters\n",
            "norm.bias has 128 trainable parameters\n",
            "final_linear.weight has 640256 trainable parameters\n",
            "final_linear.bias has 5002 trainable parameters\n",
            "The model has approximately 1947786 trainable parameters.\n"
          ]
        }
      ],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Print the parameter counts per layer\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} has {param.numel()} trainable parameters\")\n",
        "\n",
        "# Approximate model size by counting parameters\n",
        "model_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has approximately {model_parameters} trainable parameters.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sueaKjioRgPu"
      },
      "source": [
        "## **Comparison**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiNawx9Glu5s"
      },
      "source": [
        "I compared my initial models below based on perplexity scores and times. However, these might not be the most accurate due to slightly different model architectures due to tweaking during debugging each model, so in Part II, I use particular instances of Frankenstein to emulate each of the other architectures. Therefore, those results are better, but this is generally consistent with them, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QppbgbHfF7JN"
      },
      "outputs": [],
      "source": [
        "# Count number of paramaeters in a model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, data_loader, loss_fn, device, vocab_size):\n",
        "    \"\"\"\n",
        "    Calculate the perplexity of a language model, which measures how well the model predicts a sample.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model to evaluate.\n",
        "        data_loader (DataLoader): DataLoader providing input and target batches.\n",
        "        loss_fn (callable): Loss function used to evaluate model performance.\n",
        "        device (str or torch.device): Device to run the model computations on ('cpu' or 'cuda').\n",
        "        vocab_size (int): The size of the vocabulary, not used in this function directly.\n",
        "\n",
        "    Returns:\n",
        "        float: The perplexity score for the given data.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()  # Switch the model to evaluation mode (e.g., disable dropout).\n",
        "\n",
        "    total_loss = 0.0  # Initialize total loss accumulated across all batches.\n",
        "    total_items = 0  # Initialize total number of items processed (for averaging loss later).\n",
        "\n",
        "    with torch.no_grad():  # Context manager that disables gradient computation, reducing memory usage and speeding up computations.\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to the specified device.\n",
        "\n",
        "            outputs = model(inputs)  # Compute the model's output for the given inputs.\n",
        "\n",
        "            if isinstance(outputs, tuple):  # Check if model returns a tuple (e.g., output and hidden states).\n",
        "                outputs = outputs[0]  # If so, use only the logits (first element of tuple) for loss calculation.\n",
        "\n",
        "            outputs = outputs.reshape(-1, vocab_size)  # Flatten the outputs to fit the loss function's requirements.\n",
        "            targets = targets.reshape(-1)  # Flatten the targets accordingly.\n",
        "\n",
        "            loss = loss_fn(outputs, targets)  # Compute the loss between outputs and targets.\n",
        "            total_loss += loss.item() * inputs.size(0)  # Aggregate the loss scaled by batch size for correct averaging.\n",
        "            total_items += inputs.size(0)  # Accumulate the total number of items processed.\n",
        "\n",
        "    average_loss = total_loss / total_items  # Calculate the average loss over all items.\n",
        "    perplexity = torch.exp(torch.tensor(average_loss))  # Compute the perplexity from the average loss.\n",
        "\n",
        "    return perplexity.item()  # Return the perplexity score as a Python float."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b3384WFn_n0"
      },
      "source": [
        "I realized the only way to have a perfect \"control\" architecture would be to compare Frankenstein to itself. Since an MoD router with p = 1 lets all tokens through on all layers, it becomes a standard transformer. Likewise, when the number of key and value matrices are equal to query matrices, a GQ Attention system is essentially standard multi-head attention.\n",
        "I initalized 4 instances of the Frankenstein:\n",
        "- p = 1, KV_Heads = 8 (Standard Implementation)\n",
        "- P = 0.25, KV_Heads = 8 (MoD Implementation)\n",
        "- p = 1, KV_Heads = 2 (GQA Implementation)\n",
        "- p = 0.25, KV_HEADS = 2 (Hybrid Implementation)\n",
        "\n",
        "As can be seen in the results below, which vary a little by data, the hybrid implementation results in faster training than MoD or GQA alone, yet their performance in terms of perplexity is somewhere between MoD's superior performance and GQA's.\n",
        "\n",
        "The speed boost is only occuring when I train on CPUs, while on GPUs the vanilla architecture is typically slightly faster and I am assuming this is since PyTorch and CUDA kernels are optimized for running standard archtectures. It was interesting to learn that these factors that we are blind to can matter; the author of the GQA library mentions that GQA is only marginally faster than a standard transformer on PyTorch because PyTorch \"standard\" implimentations already use in-built techniques like flash attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ3EkbFeKY7Z",
        "outputId": "20d92fc4-64c0-447b-ccf7-906ae8185500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed: KV Heads = 8, Mod Reduced = 1, Time = 527.36 sec, Params = 4742538\n",
            "Training completed: KV Heads = 2, Mod Reduced = 1, Time = 317.63 sec, Params = 4149642\n",
            "Training completed: KV Heads = 8, Mod Reduced = 0.25, Time = 360.57 sec, Params = 4743562\n",
            "Training completed: KV Heads = 2, Mod Reduced = 0.25, Time = 243.19 sec, Params = 4150666\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "epochs = 3\n",
        "\n",
        "# Define unique configuration classes\n",
        "class Config1:\n",
        "    max_sequence_length = 256\n",
        "    num_tokens = 5002\n",
        "    d_model = 256\n",
        "    query_heads = 8\n",
        "    kv_heads = 8\n",
        "    dim_feedforward = 512\n",
        "    num_layers = 4\n",
        "    mod_capacity_factor = 1\n",
        "    mod_reduced = 1\n",
        "\n",
        "class Config2:\n",
        "    max_sequence_length = 256\n",
        "    num_tokens = 5002\n",
        "    d_model = 256\n",
        "    query_heads = 8\n",
        "    kv_heads = 2\n",
        "    dim_feedforward = 512\n",
        "    num_layers = 4\n",
        "    mod_capacity_factor = 1\n",
        "    mod_reduced = 1\n",
        "\n",
        "class Config3:\n",
        "    max_sequence_length = 256\n",
        "    num_tokens = 5002\n",
        "    d_model = 256\n",
        "    query_heads = 8\n",
        "    kv_heads = 8\n",
        "    dim_feedforward = 512\n",
        "    num_layers = 4\n",
        "    mod_capacity_factor = 1\n",
        "    mod_reduced = 0.25\n",
        "\n",
        "class Config4:\n",
        "    max_sequence_length = 256\n",
        "    num_tokens = 5002\n",
        "    d_model = 256\n",
        "    query_heads = 8\n",
        "    kv_heads = 2\n",
        "    dim_feedforward = 512\n",
        "    num_layers = 4\n",
        "    mod_capacity_factor = 1\n",
        "    mod_reduced = 0.25\n",
        "\n",
        "configs = [Config1, Config2, Config3, Config4]\n",
        "models = []\n",
        "train_meta = []\n",
        "\n",
        "# Training loop for each configuration class\n",
        "for Config in configs:\n",
        "    model = Frankenstein(Config()).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    total_loss = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output, aux_loss = model(inputs)\n",
        "            output = output.reshape(-1, Config.num_tokens)\n",
        "            targets = targets.reshape(-1)\n",
        "            loss = loss_fn(output, targets) + aux_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    training_duration = (datetime.now() - start_time).total_seconds()\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    models.append(model)\n",
        "    train_meta.append((training_duration, num_params))\n",
        "\n",
        "    print(f\"Training completed: KV Heads = {Config.kv_heads}, Mod Reduced = {Config.mod_reduced}, Time = {training_duration:.2f} sec, Params = {num_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5YQmAqsLxDm",
        "outputId": "bbdad871-f901-4de3-afac-9282fd183f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Name           | Training Time (s) | Number of Parameters | Perplexity\n",
            "---------------------------------------------------------------------------\n",
            "Vanilla Transformer  | 527.36            | 4742538              | 11.47     \n",
            "GQA                  | 317.63            | 4149642              | 13.18     \n",
            "MoD                  | 360.57            | 4743562              | 14.36     \n",
            "Frankenstein         | 243.19            | 4150666              | 15.58     \n"
          ]
        }
      ],
      "source": [
        "vocab_size = 5002\n",
        "\n",
        "perplexities = [\n",
        "    calculate_perplexity(models[0], test_loader, loss_fn, device, vocab_size),\n",
        "    calculate_perplexity(models[1], test_loader, loss_fn, device, vocab_size),\n",
        "    calculate_perplexity(models[2], test_loader, loss_fn, device, vocab_size),\n",
        "    calculate_perplexity(models[3], test_loader, loss_fn, device, vocab_size)\n",
        "]\n",
        "\n",
        "model_names = [\"Vanilla Transformer\", \"GQA\", \"MoD\", \"Frankenstein\"]\n",
        "results = []\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    results.append({\n",
        "        \"Model Name\": model_names[i],\n",
        "        \"Training Time (s)\": train_meta[i][0],\n",
        "        \"Number of Parameters\": train_meta[i][1],\n",
        "        \"Perplexity\": perplexities[i]\n",
        "    })\n",
        "\n",
        "# Print results in a table\n",
        "print(f\"{'Model Name':<20} | {'Training Time (s)':<17} | {'Number of Parameters':<20} | {'Perplexity':<10}\")\n",
        "print(\"-\" * 75)\n",
        "for result in results:\n",
        "    print(f\"{result['Model Name']:<20} | {result['Training Time (s)']:<17.2f} | {result['Number of Parameters']:<20} | {result['Perplexity']:<10.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-ZMVMMowsXk",
        "outputId": "0ab59c8e-aa60-4cc0-f856-6a4c7832c4a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Vanilla Transformer  | 527.36            | 4742538              | 11.47     \n",
            "GQA                  | 317.63            | 4149642              | 13.18     \n",
            "MoD                  | 360.57            | 4743562              | 14.36     \n",
            "Frankenstein         | 243.19            | 4150666              | 15.58     \n",
            "Vanilla Transformer  | 527.36            | 4742538              | 11.47     \n",
            "GQA                  | 317.63            | 4149642              | 13.18     \n",
            "MoD                  | 360.57            | 4743562              | 14.36     \n",
            "Frankenstein         | 243.19            | 4150666              | 15.58     \n"
          ]
        }
      ],
      "source": [
        "for i, model in enumerate(models):\n",
        "    # Saving the model\n",
        "    torch.save(model.state_dict(), f\"/content/{model_names[i]}_model.pth\")\n",
        "\n",
        "    results.append({\n",
        "        \"Model Name\": model_names[i],\n",
        "        \"Training Time (s)\": train_meta[i][0],\n",
        "        \"Number of Parameters\": train_meta[i][1],\n",
        "        \"Perplexity\": perplexities[i]\n",
        "    })"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZwHQlKyqtd2r",
        "5rPgX20SnRF4",
        "LjA363JaE_lS",
        "BdAU3qiK6gXq",
        "akqza-CsHgMt",
        "CRiGODMEgmyW",
        "-G9eRrfuj2p0",
        "x0DkFcA2onyg",
        "SxlvzCP1Gjf7",
        "5_2Ylv3WKRnG"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}